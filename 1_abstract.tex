\begin{abstract}
Hallucination detection aims to evaluate if the generated text is consistent with the inputs (i.e.,~faithfulness) or consistent with common sense (i.e.,~factuality).
Previous model-based hallucination detection methods usually focus on a single task (e.g.,~summarization) and a single purpose (i.e.,~faithfulness or factuality) because supervisions used for training need lots of manual annotations or heuristics rules for dataset construction. To reduce human efforts, previous work employed a closed large language model (LLM) to annotate summarization data and then trained a model-based metric. However, they still have a limited scope and 
%a closed LLM is hard to follow. 
it is unclear how open-source LLMs perform on this task.
Hence, in this paper, to have a thorough understanding of using supervision from LLMs, we first conduct a comprehensive LLM evaluation on multiple tasks and purposes. Then, with supervision from LLMs, we learn a model-based metric that can be used for both faithfulness and factuality on multiple NLP tasks. Extensive experiments show that \todo{experiments}
\end{abstract}