\section{LLM Evaluation and Analysis}
To study abilities of different open-source LLMs on evaluating faithfulness and factuality and select the best LLM as our annotator, we first conduct comprehensive experiments of LLMs on the benchmark datasets introduced in~\Cref{sec:benchmark}.
\subsection{Models}
There are various large language models proposed in previous work, and we select the most powerful models when we write this paper for evaluation and analysis:
\textbf{Open-source models}:

(1) \textbf{Dolly v2} (7B, 12B)~\cite{DatabricksBlog2023DollyV2};
(2) \textbf{Falcon} (7B, 40B)~\cite{falcon40b};
(3) \textbf{Koala} (7B, 13B)~\cite{koala_blogpost_2023};
(4) \textbf{Alpaca} (7B, 13B)~\cite{alpaca};
(5) \textbf{Vicuna} (7B, 13B)~\cite{vicuna2023};
(6) \textbf{Llama2} (7B, 13B, 70B)~\cite{Touvron2023Llama2O}.

For Falcon and Llama2, we use the instruction-finetuned version and chat version respectively, because we find their original versions will not follow our prompt to output usable answers as annotations.

\textbf{Closed-source models}:

(1) \textbf{GPT-3.5-Turbo}~\footnote{\url{https://platform.openai.com/docs/models/gpt-3-5}}; 
(2) \textbf{GPT-4}~\footnote{\url{https://platform.openai.com/docs/models/gpt-4}}.

We introduce checkpoint details of LLMs and conversational templates used for LLMs in \todo{Appendix}.

\subsection{Faithfulness}
\label{sec:llm_faithful}
\textbf{Datasets}
To efficiently evaluate LLMs on faithfulness, we randomly sample 100 examples from each dataset introduced in~\Cref{sec:bench_faithfulness}, hence, we have in total 1,300 data instances for faithfulness evaluation. 

\textbf{Experiment Settings}
We feed each instance into a zero-shot prompt template to query LLMs to get answers:

\textit{Input Text: <\textbf{grounding text}>\\
Generated Text: <\textbf{generated text}>\\
Can this generated text be inferred from the input text? Please have a thoughtful and rigorous consideration of this question and answer in ``True'' or ``False''.}

LLMs will generate answers containing ``True'' or ``False'' and we parse answers into labels 1 or 0 respectively. We use zero-shot prompting since applying few-shot prompting did not improve performance in early experiments. This observation is consistent with TrueTeacher~\cite{Gekhman2023TrueTeacherLF} results.

We treat this task as a binary classification problem and LLMs will output binary predictions ~\todo{K: Response Parsing in Appendix}. To evaluate performance, we use \textit{ungrounded} F1 scores as the evaluation metric which means we treat ungrounded label (i.e.,~label 0 in the data) as positives for F1 calculation for evaluating the ability of LLMs to detect hallucinations.




% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table*}[t]
\centering
\small
\scalebox{0.98}{
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{l|ccccc|cccc|cc|c|c|c}
\toprule
\multicolumn{1}{c|}{} &
  \multicolumn{5}{c|}{\textbf{Summarization}} &
  \multicolumn{4}{c|}{\textbf{Dialogue}} &
  \multicolumn{2}{c|}{\textbf{Fact Check}} &
  \textbf{Para.} &
  \textbf{QA} &
   \\ \cmidrule(lr){2-14}
\multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{LLMs}}} &
  Frank &
  Sum &
  MNBM &
  QAGS &
  H-sum &
  BEGIN &
  Q2 &
  DialFact &
  H-dial &
  Fever &
  VitaC &
  PAWs &
  H-QA &
  \multirow{-2}{*}{\textbf{Overall}} \\ \midrule
Dolly v2-7B  & 0.11  & 0.07 & 0.14 & 0.11 & 0.05  & 0.14  & 0.32 & 0.17     & 0.25   & 0.40  & 0.21  & 0.29 & 0.27 & 0.20 \\
Dolly v2-12B & 0.50  & 0.39 & 0.32 & 0.29 & 0.28  & 0.37  & 0.49 & 0.57     & 0.42   & 0.66  & 0.46  & 0.38 & 0.38 & 0.45 \\
Falcon-7B    & 0.17  & 0.07 & 0.26 & 0.20 & 0.23  & 0.16  & 0.20 & 0.10     & 0.27   & 0.23  & 0.20  & 0.26 & 0.16 & 0.20 \\
Falcon-40B   & 0.71  & 0.34 & 0.63 & 0.46 & 0.40  & 0.54  & 0.36 & 0.62     & 0.50   & 0.63  & 0.46  & 0.60 & 0.44 & 0.53 \\
Koala-7B     & 0.60  & 0.22 & 0.46 & 0.27 & 0.18  & 0.46  & 0.55 & 0.61     & 0.35   & 0.64  & 0.47  & 0.48 & 0.26 & 0.46 \\
Koala-13B    & 0.69  & 0.26 & 0.79 & 0.53 & 0.53  & 0.70  & 0.60 & 0.74     & 0.59   & 0.79  & 0.59  & 0.66 & 0.46 & 0.63 \\
Alpaca-7B    & 0.55  & \textbf{0.49} & 0.57 & 0.55 & 0.16  & 0.55  & 0.55 & 0.76     & 0.39   & 0.78  & 0.54  & 0.44 & 0.48 & 0.55 \\
Alpaca-13B   & 0.70  & 0.39 & 0.74 & 0.52 & 0.45  & 0.55  & 0.64 & 0.74     & 0.63   & 0.77  & 0.41  & 0.60 & 0.54 & 0.61 \\
Vicuna-7B    & 0.50  & 0.20 & 0.58 & 0.28 & 0.29  & 0.53  & 0.60 & 0.62     & 0.38   & 0.75  & 0.63  & 0.48 & 0.42 & 0.52 \\
Vicuna-13B   & 0.81  & 0.31 & 0.77 & 0.64 & 0.49  & 0.61  & 0.65 & \textbf{0.86}     & 0.68   & 0.84  & 0.64  & 0.67 & 0.75 & 0.70 \\
Llama2-7B    & 0.81  & 0.33 & 0.86 & 0.65 & 0.59  & 0.76  & 0.58 & 0.74     & 0.55   & 0.83  & 0.61  & 0.74 & 0.68 & 0.70 \\
Llama2-13B   & \textbf{0.87}  & 0.38 & \textbf{0.95} & \textbf{0.69} & \textbf{0.66}  & 0.60  & 0.60 & 0.77     & \textbf{0.70}   & 0.81  & \textbf{0.71}  & 0.76 & 0.73 & \textbf{0.73} \\
Llama2-70B   & 0.71  & 0.41 & 0.78 & 0.57 & 0.43  & \textbf{0.75}  & \textbf{0.69} & 0.83     & 0.68   & \textbf{0.86}  & 0.68  & \textbf{0.84} & \textbf{0.76} & \textbf{0.73} \\ \midrule
GPT3.5       & \underline{0.87}  & 0.47 & \underline{0.87} & \underline{0.71} & \underline{0.61}  & 0.77  & 0.71 & 0.82     & 0.68   & 0.88  & 0.65  & 0.69 & 0.76 & 0.75 \\
GPT4         & \underline{0.87}  & \underline{0.50} & 0.79 & 0.67 & 0.33  & \underline{0.78}  & \underline{0.75} & \underline{0.88}     & \underline{0.74}   & \underline{0.89}  & \underline{0.70}  & \underline{0.80} & \underline{0.83} & \underline{0.80} \\ \bottomrule
\end{tabular}
}}
\caption{The performance (F1 scores) of faithfulness evaluation using different LLMs over 13 datasets. The best score on each dataset is \textbf{bold} for open-source LLMs and \underline{underlined} for closed-source LLMs.}
\label{tab:llm_eval_faithful}
\end{table*}

\textbf{Result Analysis}


\subsection{Factuality}
\textbf{Datasets}
Similar as faithfulness evaluation, we randomly sample 100 examples from different factuality datasets. For TrueFalse dataset, we sample 100 examples from each category. We have 1,000 data instances for LLM evaluation in total.

\textbf{Experiment Settings}
We feed each instance into a zero-shot prompt template to query LLMs to get answers:

\textit{Generated Text: <\textbf{generated text}>\\
Is the generated text consistent to the common sense or facts in the world? Please have a thoughtful and rigorous consideration of this question and answer in ``True'' or ``False''.}

Similarly, LLMs generate answers containing ``True'' or ``False'' and we parse answers into labels 1 or 0 respectively. We do not see improvements with few-shot prompts for this task.


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table*}[]
\centering
\small
\scalebox{0.98}{
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{l|ccccccccc|c}
\toprule
\multicolumn{1}{c|}{\textbf{LLMs}} &
  \textbf{Wiki bio} &
  \textbf{Halu general} &
  \textbf{Animals} &
  \textbf{Capitals} &
  \textbf{Cities} &
  \textbf{Companies} &
  \textbf{Elements} &
  \textbf{Facts} &
  \textbf{Inventions} &
  \textbf{Overall} \\ \midrule
Dolly v2-7B  & 0.00 & 0.17 & 0.12 & 0.00 & 0.12 & 0.06 & 0.16 & 0.23 & 0.14 & 0.11 \\
Dolly v2-12B & 0.46 & 0.00 & 0.45 & 0.60 & 0.34 & 0.66 & 0.22 & 0.41 & 0.42 & 0.40 \\
Falcon-7B    & 0.34 & 0.37 & 0.52 & 0.32 & 0.39 & 0.52 & 0.41 & 0.50 & 0.38 & 0.42 \\
Falcon-40B   & 0.66 & 0.35 & 0.75 & 0.64 & 0.70 & 0.72 & 0.64 & 0.71 & 0.71 & 0.65 \\
Koala-7B     & 0.37 & 0.23 & 0.51 & 0.60 & 0.52 & 0.55 & 0.53 & 0.56 & 0.53 & 0.49 \\
Koala-13B    & 0.50 & 0.23 & 0.56 & 0.61 & 0.74 & 0.73 & 0.65 & 0.70 & 0.72 & 0.60 \\
Alpaca-7B    & 0.36 & 0.36 & 0.65 & 0.69 & 0.75 & 0.74 & 0.60 & 0.74 & 0.58 & 0.61 \\
Alpaca-13B   & 0.20 & 0.23 & 0.62 & 0.66 & 0.67 & 0.76 & 0.62 & 0.75 & 0.64 & 0.57 \\
Vicuna-7B    & 0.53 & 0.27 & 0.65 & 0.58 & 0.65 & 0.74 & 0.63 & 0.73 & 0.66 & 0.60 \\
Vicuna-13B   & 0.38 & 0.17 & 0.74 & 0.77 & 0.77 & 0.82 & 0.68 & 0.79 & 0.75 & 0.65 \\
Llama2-7B    & 0.09 & 0.22 & 0.67 & 0.58 & 0.68 & 0.76 & 0.69 & 0.77 & 0.65 & 0.57 \\
Llama2-13B   & 0.11 & 0.17 & 0.82 & 0.93 & 0.91 & 0.83 & 0.73 & 0.82 & 0.85 & 0.69 \\
Llama2-70B   & 0.51 & 0.31 & 0.78 & 0.82 & 0.75 & 0.76 & 0.65 & 0.86 & 0.70 & 0.68 \\ \midrule
GPT3.5       & 0.11 & 0.17 & 0.86 & 0.95 & 0.87 & 0.87 & 0.88 & 0.87 & 0.91 & 0.72 \\
GPT4         & 0.68 & 0.26 & 0.84 & 0.93 & 0.93 & 0.91 & 0.96 & 0.94 & 0.95 & 0.82 \\ \bottomrule
\end{tabular}
}}
\caption{The performance (F1 scores) of factuality evaluation using different LLMs over 9 datasets. The best score on each dataset is \textbf{bold} for open-source LLMs and \underline{underlined} for closed-source LLMs.}
\label{tab:llm_eval_factuality}
\end{table*}


\textbf{Result Analysis}